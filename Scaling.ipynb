{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaICuYvnJbJbrg65emMHC5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Normalisation:\n","\n","it helps you to scale down your feature between 0 and 1.\n","\n","it is also called MinMaxScaler\n","\n","Standardization:\n","\n","it helps you to scale down your feature based on standard normal distribution.\n","\n","Here standard deviation is 1 and mean is usually 0.\n","\n","Library used for standardization is called StandardScaler\n","z=(x-u)/s\n","where 'u' is mean of the training samples, 's' is the standard deviation of the training samples and x is the sample."],"metadata":{"id":"_cPBhUxPgc38"}},{"cell_type":"markdown","source":["Advantages of standardization:\n","*  Comparable data: Makes it easier to compare data points with different units or scales.\n","*  Improved Model Performance: Helps machine learning models converage faster and perform better, especially with algorithms that assume data follows a normal distribution.\n","*  Stabilizes Learning: Reduces the influence of outliers and ensures each feature contributes equally.\n","\n","\n","Advantages of Normalization:\n","* Equal Contribution: scales all features to a common range, ensuring no single feature dominates the learning process.\n","*  Enhanced algorithm performance: Particularly beneficial for algorithms like neural networks and k-nearest neighbors, where input scales can impact performance.\n","*  Maintain Data integrity: Keeps the original data distribution intact while bringing all features to the same scale."],"metadata":{"id":"55JkIlkUkNkO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDsRzoVsgZJ1"},"outputs":[],"source":[]}]}